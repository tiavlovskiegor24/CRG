#+STARTUP: indent

Managing my work at CRG 


* BHIVE
** March 17
*** Mon 13
**** DONE Only leave '_r' scores, drop the rest      :bhive:ds_processing:
CLOSED: [2017-03-13 Mon 15:59]

***** DONE Include this features in the [[file:feature_types.py][feature types]]
CLOSED: [2017-03-13 Mon 14:46]
***** DONE Visualise and choose preprocessing of this types
CLOSED: [2017-03-13 Mon 15:08]
Eduard said that gaussian normalisation would be fine
**** DONE Think through more discreet handling of samples with Nans :bhive:ds_processing:
CLOSED: [2017-03-13 Mon 15:11]
***** DONE for example leave those samples which have Nan for distance features
CLOSED: [2017-03-10 Fri 17:25]
substituting distance Nan with 10*max distance value in dataset
quivalent to "not on this chromosome"

***** DONE move the issue to the feature_type processing [[file:feature_types.py][rules]] 
CLOSED: [2017-03-10 Fri 17:27]
feature type preprocessing removes certain Nans (using rules) and leaves the others 
untouched thus allowing the entire sample to be removed from the dataset
After feature_type preproicessing all the samples with Nans are removed befor passing
them to machine learning 

****** DONE Consider not removing samples with Nans but masking them instead
CLOSED: [2017-03-13 Mon 14:24] SCHEDULED: <2017-03-13 Mon>
simply store the indices of non-Nan samples which will be passed to ml optimization
**** DONE create control file for the form_full_dataset and prepare_ML_inputs routines 
CLOSED: [2017-03-13 Mon 16:01]
**** DONE Consider not dropping rows and columns of dataset for ML_inputs but creating masks
CLOSED: [2017-03-13 Mon 14:25] SCHEDULED: <2017-03-13 Mon>
*** Tue 14 
**** DONE Move target values selection and processing to control file
CLOSED: [2017-03-14 Tue 14:01]
**** DONE Generate classification report for predictions on test dataset using best param classifier :bhive:ml_pipeline:
CLOSED: [2017-04-03 Mon 14:16] SCHEDULED: <2017-03-23 Thu 12:00>

***** DONE Return classification report only for classifiers
CLOSED: [2017-04-03 Mon 14:37] SCHEDULED: <2017-03-29 Wed>
**** DONE Include preprocessing functions for feature types in [[file:feature_types.py][feature_types]] :bhive:features:ds_processing:
CLOSED: [2017-03-20 Mon 14:05] SCHEDULED: <2017-03-15 Wed 13:00>
***** DONE Hi-c features
CLOSED: [2017-03-29 Wed 16:06]
****** DONE GMFPT
CLOSED: [2017-03-16 Thu 15:44]
****** DONE Row sum
CLOSED: [2017-03-15 Wed 17:10]
****** DONE contact decay
CLOSED: [2017-03-16 Thu 16:00]
****** DONE AB-score
CLOSED: [2017-03-16 Thu 16:55]
****** DONE Intra-inter_ratio
CLOSED: [2017-03-16 Thu 16:41]
***** DONE Chip-c features
CLOSED: [2017-03-20 Mon 14:05]
****** DONE zb_r
CLOSED: [2017-03-17 Fri 14:32] SCHEDULED: <2017-03-17 Fri 14:00>
first take the max value from [[file:~/CRG/Datasets/Jurkat_gws_50kb.txt][this set]] and hiv combined and scale with this one
i think for this as long as I use the same scaling factor for training and prediction
the value of the factor can be picked to falcilitate training
****** DONE hb_r
CLOSED: [2017-03-20 Mon 14:05]
rescaling is required
***** DONE Display how many samples are Nan (excluded for each feature type)
CLOSED: [2017-03-20 Mon 13:55]
**** DONE Count how many nan targets are present and add them to samples mask
CLOSED: [2017-03-16 Thu 18:05]

**** DONE Add gradient boosting to ML_pipeline
CLOSED: [2017-03-23 Thu 12:52] SCHEDULED: <2017-03-20 Mon>
**** DONE Add nearest neighbours to ML_pipelines
CLOSED: [2017-03-23 Thu 12:52]
**** DONE Moving the scalling of features to the DS preprocessing stage :bhive:ds_processing:ml_inputs:
CLOSED: [2017-03-16 Thu 18:12]
move the issue to the feature_type processing [[file:feature_types.py][rules]] 
**** DONE Consider changing the present feature type mapping in dataset :ds_processing:
CLOSED: [2017-04-03 Mon 14:19]
For example check for present feature_types by looking through dataset column names and
checking "_? _" segments of the name the feature type id look up table
**** DONE Include categorical feature type into [[file:feature_types.py][our feature dictionary]]
CLOSED: [2017-04-03 Mon 14:22]
**** DONE Switch back to regression on expression level
CLOSED: [2017-04-03 Mon 14:35]
**** DONE Consider using boolean masks for ML_inputs     :bhive:ml_inputs:
CLOSED: [2017-03-20 Mon 16:02] SCHEDULED: <2017-03-15 Wed 13:00>
**** DONE Consider maskig categorical features instead of [[file:dataset_processing.py::68][dropping]] them:bhive:ml_inputs:
CLOSED: [2017-03-16 Thu 18:16]
**** DONE List non processed and/or non-removed features
CLOSED: [2017-03-20 Mon 16:02] SCHEDULED: <2017-03-16 Thu 16:30>
**** DONE Rescale distance features to 0-1
CLOSED: [2017-03-16 Thu 16:56]
**** DONE Scale target values to 0-1
CLOSED: [2017-03-16 Thu 18:08]
**** DONE Consider tail packing for target values
CLOSED: [2017-04-03 Mon 14:35] SCHEDULED: <2017-03-17 Fri>
yes lower 1 percent and upper 5 percent
simple linear scaling to 1 percent of inner 98% range
for now not using it
**** DONE instantiate feature_type preprocessing as objects
CLOSED: [2017-03-23 Thu 10:29] SCHEDULED: <2017-03-20 Mon>
so that they preserve scaling and normalisation parameters for training set
to be used on test set and subsequent predicitons
***** DONE Store the object in the dict for future use
CLOSED: [2017-03-21 Tue 13:57]
stored in 'processing field of ML_inputs'
SCHEDULED: <2017-03-20 Mon>
***** DONE Include test preprocessing in ML inputs
CLOSED: [2017-03-21 Tue 13:59] SCHEDULED: <2017-03-21 Tue>
***** DONE Test the process objects on test and train
CLOSED: [2017-03-23 Thu 10:29] SCHEDULED: <2017-03-20 Mon>
****** DONE gmfpt
CLOSED: [2017-03-21 Tue 13:59]
****** DONE row_sum
CLOSED: [2017-03-23 Thu 10:28]
****** DONE ab_score
CLOSED: [2017-03-23 Thu 10:28]
****** DONE distances
CLOSED: [2017-03-23 Thu 10:28]
****** DONE contact_decay
CLOSED: [2017-03-23 Thu 10:28]
****** DONE chip-c featu
CLOSED: [2017-03-23 Thu 10:28]
**** DONE Introduce tail compaction below 1% and above 99% of data points for outlier removal 
CLOSED: [2017-03-23 Thu 10:29] SCHEDULED: <2017-03-21 Tue>
upper = 99nt percentile
lower = 1st percentile
mean_sep = (upper-lower)/(n*0.98-1)
sort top and bottow percent of samples
prev = upper
for each sample > upper in sorted order:
    sample_old = sample
    next = sample
    sep_d = (next-prev-mean_sep)
    if sep_d > 0
        fraction = sep_d/mean_sep
        sample_new = mean_sep*(1+log(1+fraction))
    else:
        sample_new = sample_old
    prev = sample_old
then the same for lower 1 percent


!!!Important!!! neighbour separation based compaction does not work because the same
point will get mapped to different values depending on how many points come before him

The better way is to simply compress the separation distance linearly 
from the benchmark percentile
compression ration can be equivalent to 1% of internal  98% range
**** DONE Create linear tail compaction in [[file:auxiliary_items.py][auxiliary functions]]
CLOSED: [2017-03-21 Tue 14:01]
***** DONE Embedd this linear function to all features
CLOSED: [2017-03-23 Thu 10:29] SCHEDULED: <2017-03-22 Wed>
****** DONE gmfpt
CLOSED: [2017-03-21 Tue 14:01]
****** DONE Row sum
CLOSED: [2017-03-22 Wed 11:46]
****** DONE Contact decay
CLOSED: [2017-03-22 Wed 11:47]
****** DONE inter_intra
CLOSED: [2017-03-22 Wed 11:48]
****** DONE chip-c features
CLOSED: [2017-03-22 Wed 13:41]
**** DONE Consider using 3 class clasification (low, medium,high exression)
CLOSED: [2017-04-03 Mon 14:35] DEADLINE: <2017-03-24 Fri>

***** DONE Create function for generating 3 class targets
CLOSED: [2017-03-27 Mon 17:20] SCHEDULED: <2017-03-20 Mon>
**** DONE Move target creation and preprocessing to feature_types
CLOSED: [2017-03-23 Thu 17:02] SCHEDULED: <2017-03-23 Thu>
stored in dictionary target_types
Random Forest is by default a multiclass

**** DONE Run classifiers without 'distance features'
CLOSED: [2017-04-03 Mon 14:35] SCHEDULED: <2017-03-24 Fri>
** TODO Run estimators with chip-c features only
SCHEDULED: <2017-03-29 Wed>

** TODO Run estimators on random data 
SCHEDULED: <2017-03-30 Thu>
import numpy as np
data = np.random.random((1000, 784))
labels = np.random.randint(2, size=(1000, 1))

** DONE Run estimators on randomly shuffled targets                :results:
CLOSED: [2017-04-03 Mon 14:41] SCHEDULED: <2017-03-30 Thu>
the results were worse than for the correct targets
meaning that my pipeline does not mess up anything
and there is some information present
but it is only 3-6% higher than randomly shuffled
e.i. our features are not very informative 

** TODO Run estimators on randomly sampled targets
SCHEDULED: <2017-03-30 Thu>

** TODO Identify the syntetic target in the raw file             :debugging:
SCHEDULED: <2017-03-31 Fri>
to see that pipline does not accidentlay shuffle or alter the data

I added [[file:ML_inputs.py::179][control targets]] column to my datasets which simply is a linear combination of other features
in the end if I choose [[file:target_types.py::10][in_dataset]] target type the machine learning pipleine should easily identify
the linear dependence of targets on features

** TODO Create Adaboost pipeline                         :bhive:ml_pipeline:
** DONE Add [[http://scikit-learn.org/stable/modules/multiclass.html][multiclass estimator]] to ML_estimators
CLOSED: [2017-04-03 Mon 17:22] SCHEDULED: <2017-03-28 Tue>
random forest already have multiclass capability
** TODO Integration site prediction 
DEADLINE: <2017-04-07 Fri>
*** TODO Form full dataset from gmfpt, row_sum and contact decay
SCHEDULED: <2017-04-04 Tue>
*** TODO Create control file for this problem
SCHEDULED: <2017-04-04 Tue>
*** TODO form ML_inputs from full dataser 
SCHEDULED: <2017-04-05 Wed>
*** TODO run ML_process on ML_inputs
SCHEDULED: <2017-04-05 Wed>


* Task log 

** DONE Check CRG email
CLOSED: [2017-03-24 Fri 13:45] SCHEDULED: <2017-03-23 Thu 13:30 +1d>

* CRG Events
** DONE Seminar on genetics
CLOSED: [2017-03-28 Tue 18:05] SCHEDULED: <2017-03-22 Wed 12:00-13:00>

* Lab events
** Journal Club                                               :journal_club:
*** My journal club
SCHEDULED: <2017-04-20 Thu 14:00 -2w>

*** DONE Prepare the [[https://s3.amazonaws.com/prod_object_assets/assets/302475040714132/Transposable_elements_have_rewired_the_core_regulatory.pdf?AWSAccessKeyId=ASIAIF2K6UIW3CKOANDQ&Expires=1490716688&Signature=cBYTWWL%252BJmnljo2KS9A2MT5xCCA%253D&x-amz-security-token=FQoDYXdzEDEaDA14epkSLWLQMOKYWyK3A1fQs2qVEALdFHrCpvPo0GqKGJ433onTfZ8NtApTfbYbQJm79oU%252BM43ywZbjKlStbF7oFk3zYfoZhatFzuomxWCgl5CNYkdqn4eORTIufyCb%252F34aIX8E%252F%252Bfe%252FfgLlrmYTJ3i8BSlJF6%252BOVlCzLRDWXr35B%252FQgWVVeUaJKzWHBCD2Kxq5PKyZSBGTtV8bxtZ%252BRlwl0Gi2j5Kl9nAI7woznRSp8UmKwY3mUVCHerLIyApokAfBTuFy92A8CDItFSREkRlcHamGZmzrbY57OBW%252FVcgkOHyTK%252B%252FqFRjzmUn5mHEQKyXJd3kbR4ij7H67SkNxuz4b66WmdMv49ZsCHUrdFddtUo1VAPX1nlL%252BCbis3gk1HE29QVaZJgI2%252FuEp1RDDyl0RHfhrwy756J%252BFhd%252Foi2GZW6jwssWa9AluT1sI3yE%252FIL45FCToVCxLbYYa3tO7lmG0cuDumroxr9vdX0em2v2NH%252BD8y%252BR32iRSGYRIq%252Bcg0lm6btvf%252B7LAYDtj9ALjDR9XZTf%252F5A97azSqIdJBUCa5YF%252BhRNTgiGnVd9Z9LhcnWxL%252F4FvcH4OiUQJnzJRTb5rxXod8pGAog4HqxgU%253D#_=_][paper]] 
CLOSED: [2017-04-03 Mon 14:37] DEADLINE: <2017-03-30 Thu 13:00> SCHEDULED: <2017-03-29 Wed>
* BHIVE (archive)** Analysis
*** HIV expression prediciton
**** Run SVMs
**** Run RF
**** Run AdaBoost
**** Run gradient boosting



** Machine Learning Pipeline
*** DONE Create SVM pipeline function 
*** DONE Test SVM pipeline function
    Seems to be working


** Dataset processing
*** DONE drop features
*** DONE Split into train and test and save to files
*** DONE count number of samples with NaNs
    Number of samples with Nan is 433
    For now I simply deleted those samples

**** DONE remove Nans from the data set and train on those 

*** DONE [#C] create ML_inputs namedtuple rather then dictionary
*** DONE [#A] extract and store indices of different features types in ML_inputs tuple
*** DONE [#A] apply log1p to the distance values



** Feature File processing
*** DONE Finish up feature file to full array routine
*** DONE finish off write feature file


** Hi-C matrix features
*** DONE fix GMFPT feature writing to file
*** DONE check row sum feature writting to file
*** DONE check decay constant writting to file
*** DONE compute gmfpt 

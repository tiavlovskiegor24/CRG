#+STARTUP: indent

Managing my work at CRG 


* Task log 

** March 17

*** Fri 10

*** Mon 13
**** DONE Only leave '_r' scores, drop the rest      :bhive:ds_processing:
CLOSED: [2017-03-13 Mon 15:59]

***** DONE Include this features in the [[file:feature_types.py][feature types]]
CLOSED: [2017-03-13 Mon 14:46]
***** DONE Visualise and choose preprocessing of this types
CLOSED: [2017-03-13 Mon 15:08]
Eduard said that gaussian normalisation would be fine
**** DONE Think through more discreet handling of samples with Nans :bhive:ds_processing:
CLOSED: [2017-03-13 Mon 15:11]
***** DONE for example leave those samples which have Nan for distance features
CLOSED: [2017-03-10 Fri 17:25]
substituting distance Nan with 10*max distance value in dataset
quivalent to "not on this chromosome"

***** DONE move the issue to the feature_type processing [[file:feature_types.py][rules]] 
CLOSED: [2017-03-10 Fri 17:27]
feature type preprocessing removes certain Nans (using rules) and leaves the others 
untouched thus allowing the entire sample to be removed from the dataset
After feature_type preproicessing all the samples with Nans are removed befor passing
them to machine learning 

****** DONE Consider not removing samples with Nans but masking them instead
CLOSED: [2017-03-13 Mon 14:24] SCHEDULED: <2017-03-13 Mon>
simply store the indices of non-Nan samples which will be passed to ml optimization
**** DONE create control file for the form_full_dataset and prepare_ML_inputs routines 
CLOSED: [2017-03-13 Mon 16:01]
**** DONE Consider not dropping rows and columns of dataset for ML_inputs but creating masks
CLOSED: [2017-03-13 Mon 14:25] SCHEDULED: <2017-03-13 Mon>
*** Tue 14 
**** DONE Move target values selection and processing to control file
CLOSED: [2017-03-14 Tue 14:01]
**** TODO Generate classification report for predictions on test dataset using best param classifier :bhive:ml_pipeline:
SCHEDULED: <2017-03-23 Thu 12:00>
**** DONE Include preprocessing functions for feature types in [[file:feature_types.py][feature_types]] :bhive:features:ds_processing:
CLOSED: [2017-03-20 Mon 14:05] SCHEDULED: <2017-03-15 Wed 13:00>
***** DONE Hi-c features
CLOSED: [2017-03-16 Thu 16:56]
****** DONE GMFPT
CLOSED: [2017-03-16 Thu 15:44]
****** DONE Row sum
CLOSED: [2017-03-15 Wed 17:10]
****** DONE contact decay
CLOSED: [2017-03-16 Thu 16:00]
****** DONE AB-score
CLOSED: [2017-03-16 Thu 16:55]
****** DONE Intra-inter_ratio
CLOSED: [2017-03-16 Thu 16:41]
***** DONE Chip-c features
CLOSED: [2017-03-20 Mon 14:05]
****** DONE zb_r
CLOSED: [2017-03-17 Fri 14:32] SCHEDULED: <2017-03-17 Fri 14:00>
first take the max value from [[file:~/CRG/Datasets/Jurkat_gws_50kb.txt][this set]] and hiv combined and scale with this one
i think for this as long as I use the same scaling factor for training and prediction
the value of the factor can be picked to falcilitate training
****** DONE hb_r
CLOSED: [2017-03-20 Mon 14:05]
rescaling is required
***** DONE Display how many samples are Nan (excluded for each feature type)
CLOSED: [2017-03-20 Mon 13:55]
**** DONE Count how many nan targets are present and add them to samples mask
CLOSED: [2017-03-16 Thu 18:05]
**** TODO Create Adaboost pipeline                     :bhive:ml_pipeline:
**** TODO Add gradient boosting to ML_pipeline
SCHEDULED: <2017-03-20 Mon>
**** DONE Moving the scalling of features to the DS preprocessing stage :bhive:ds_processing:ml_inputs:
CLOSED: [2017-03-16 Thu 18:12]
move the issue to the feature_type processing [[file:feature_types.py][rules]] 
**** TODO Consider changing the present feature type mapping in dataset :ds_processing:
For example check for present feature_types by looking through dataset column names and
checking "_? _" segments of the name the feature type id look up table
**** TODO Include categorical feature type into [[file:feature_types.py][our feature dictionary]]
**** TODO Swithc back to regression on expression level
**** DONE Consider using boolean masks for ML_inputs     :bhive:ml_inputs:
CLOSED: [2017-03-20 Mon 16:02] SCHEDULED: <2017-03-15 Wed 13:00>
**** DONE Consider maskig categorical features instead of [[file:dataset_processing.py::68][dropping]] them:bhive:ml_inputs:
CLOSED: [2017-03-16 Thu 18:16]
**** DONE List non processed and/or non-removed features
CLOSED: [2017-03-20 Mon 16:02] SCHEDULED: <2017-03-16 Thu 16:30>
**** DONE Rescale distance features to 0-1
CLOSED: [2017-03-16 Thu 16:56]
**** DONE Scale target values to 0-1
CLOSED: [2017-03-16 Thu 18:08]
**** TODO Consider tail packing for target values
SCHEDULED: <2017-03-17 Fri>
yes lower 1 percent and upper 5 percent
simple linear scaling to 1 percent of inner 98% range
**** DONE instantiate feature_type preprocessing as objects
CLOSED: [2017-03-21 Tue 13:58] SCHEDULED: <2017-03-20 Mon>
so that they preserve scaling and normalisation parameters for training set
to be used on test set and subsequent predicitons
***** DONE Store the object in the dict for future use
CLOSED: [2017-03-21 Tue 13:57]
stored in 'processing field of ML_inputs'
SCHEDULED: <2017-03-20 Mon>
***** DONE Include test preprocessing in ML inputs
CLOSED: [2017-03-21 Tue 13:59] SCHEDULED: <2017-03-21 Tue>
***** TODO Test the process objects on test and train
SCHEDULED: <2017-03-20 Mon>
****** DONE gmfpt
CLOSED: [2017-03-21 Tue 13:59]
****** TODO row_sum
****** TODO ab_score
****** TODO distances
****** TODO contact_decay
****** TODO chip-c featu
**** TODO Introduce tail compaction below 1% and above 99% of data points for outlier removal 
SCHEDULED: <2017-03-21 Tue>
upper = 99nt percentile
lower = 1st percentile
mean_sep = (upper-lower)/(n*0.98-1)
sort top and bottow percent of samples
prev = upper
for each sample > upper in sorted order:
    sample_old = sample
    next = sample
    sep_d = (next-prev-mean_sep)
    if sep_d > 0
        fraction = sep_d/mean_sep
        sample_new = mean_sep*(1+log(1+fraction))
    else:
        sample_new = sample_old
    prev = sample_old
then the same for lower 1 percent


!!!Important!!! neighbour separation based compaction does not work because the same
point will get mapped to different values depending on how many points come before him

The better way is to simply compress the separation distance linearly 
from the benchmark percentile
compression ration can be equivalent to 1% of internal  98% range
**** DONE Create linear tail compaction in [[file:auxiliary_items.py][auxiliary functions]]
CLOSED: [2017-03-21 Tue 14:01]
***** TODO Embedd this linear function to all features
SCHEDULED: <2017-03-22 Wed>
****** DONE gmfpt
CLOSED: [2017-03-21 Tue 14:01]
****** TODO Row sum
**** TODO Consider using 3 class clasification (low, medium,high exression)
DEADLINE: <2017-03-24 Fri>

***** TODO Create function for generating 3 class targets
SCHEDULED: <2017-03-20 Mon>

** TODO Check CRG email
SCHEDULED: <2017-03-21 Tue 13:30 
* Events at CRG
** Seminar on genetics
SCHEDULED: <2017-03-22 Wed 12:00-13:00>
* BHIVE (archive)** Analysis
*** HIV expression prediciton
**** Run SVMs
**** Run RF
**** Run AdaBoost
**** Run gradient boosting



** Machine Learning Pipeline
*** DONE Create SVM pipeline function 
*** DONE Test SVM pipeline function
    Seems to be working


** Dataset processing
*** DONE drop features
*** DONE Split into train and test and save to files
*** DONE count number of samples with NaNs
    Number of samples with Nan is 433
    For now I simply deleted those samples

**** DONE remove Nans from the data set and train on those 

*** DONE [#C] create ML_inputs namedtuple rather then dictionary
*** DONE [#A] extract and store indices of different features types in ML_inputs tuple
*** DONE [#A] apply log1p to the distance values



** Feature File processing
*** DONE Finish up feature file to full array routine
*** DONE finish off write feature file


** Hi-C matrix features
*** DONE fix GMFPT feature writing to file
*** DONE check row sum feature writting to file
*** DONE check decay constant writting to file
*** DONE compute gmfpt 
